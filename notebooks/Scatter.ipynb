{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About** : This notebook is used to train detection models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext nb_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../src/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import ast\n",
    "import glob\n",
    "import json\n",
    "import yaml\n",
    "import shutil\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from params import *\n",
    "\n",
    "from inference.yolox import retrieve_yolox_model, predict, YoloXWrapper\n",
    "from inference.utils import get_transfos, InferenceDataset, nms\n",
    "\n",
    "from util.plots import *\n",
    "from util.metrics import *\n",
    "from util.torch import seed_everything\n",
    "from util.boxes import Boxes\n",
    "\n",
    "from post_process.retrieve import retrieve_missing_boxes\n",
    "from post_process.remove import remove_outlier_boxes\n",
    "from post_process.reg import rounding, linear_regression\n",
    "from post_process.ticks import restrict_on_line, assign\n",
    "from post_process.in_graph import post_process_preds\n",
    "from post_process.tick_point import post_process_arrow, post_process_point_as_tick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/df_train.csv')\n",
    "df_target = pd.read_csv('../input/y_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['id'].isin(ANOMALIES)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split = pd.read_csv('../input/df_split.csv')\n",
    "df = df.merge(df_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = [\"scatter\",]\n",
    "df = df[df['chart-type'].isin(CLASSES)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigMarker:\n",
    "    selected_model = \"yolo\"\n",
    "    bbox_format = \"yolo\"\n",
    "    pred_format = \"pascal_voc\"\n",
    "\n",
    "    name = \"benetech_1_m_1\"\n",
    "    cfg = f\"../yolox/exps/{name}.py\"\n",
    "    ckpt = f\"../yolox/YOLOX_outputs/{name}/best_ckpt.pth\"\n",
    "    \n",
    "    version = \"v13\"\n",
    "#     version = \"v13_sim\"\n",
    "    labels = [\"point\"]\n",
    "\n",
    "    size = (1024, 1024)\n",
    "\n",
    "    # NMS\n",
    "    conf_thresh = 0.6\n",
    "    iou_thresh = 0.4\n",
    "    max_per_img = 500\n",
    "    min_per_img = 1\n",
    "    \n",
    "    val_bs = 1  # if size[0] > 1024 else 16\n",
    "    device = \"cuda\"\n",
    "    \n",
    "config_marker = ConfigMarker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_marker = retrieve_yolox_model(config_marker.cfg, config_marker.ckpt)\n",
    "model_marker = YoloXWrapper(model_marker, config_marker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigMarker2:\n",
    "    selected_model = \"yolo\"\n",
    "    bbox_format = \"yolo\"\n",
    "    pred_format = \"pascal_voc\"\n",
    "\n",
    "    name = \"benetech_1_l_1\"\n",
    "    cfg = f\"../yolox/exps/{name}.py\"\n",
    "    ckpt = f\"../yolox/YOLOX_outputs/{name}/best_ckpt.pth\"\n",
    "    \n",
    "    version = \"v13\"\n",
    "#     version = \"v13_sim\"\n",
    "    labels = [\"point\"]\n",
    "\n",
    "    size = (1024, 1024)\n",
    "\n",
    "    # NMS\n",
    "    conf_thresh = 0.55\n",
    "    iou_thresh = 0.4\n",
    "    max_per_img = 500\n",
    "    min_per_img = 1\n",
    "    \n",
    "    val_bs = 1  # if size[0] > 1024 else 16\n",
    "    device = \"cuda\"\n",
    "    \n",
    "config_marker_2 = ConfigMarker2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_marker_2 = retrieve_yolox_model(config_marker_2.cfg, config_marker_2.ckpt)\n",
    "model_marker_2 = YoloXWrapper(model_marker_2, config_marker_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_types = [\"scatter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = True\n",
    "PAD_ADV = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = df[df['split'] == \"val\"].reset_index(drop=True)\n",
    "df_val['path'] = f'../input/{config_marker.version}/val2017/' + df_val['id'] + '.jpg'\n",
    "df_val['gt_path'] = f'../input/{config_marker.version}/labels/valid/' + df_val['id'] + '.txt'\n",
    "df_val_ = df_val.copy()\n",
    "\n",
    "merged_boxes_list, confs_list = [], []\n",
    "for t in chart_types:\n",
    "    print(f'\\n-> Chart type : {t}\\n')\n",
    "    df_val = df_val_[df_val_['chart-type'] == t].reset_index(drop=True)  # .head(10)\n",
    "\n",
    "    print('- Predict 1')\n",
    "    transforms = get_transfos(size=config_marker.size)\n",
    "    dataset = InferenceDataset(df_val, transforms, pad=PAD, pad_advanced=PAD_ADV)\n",
    "    meter, _ = predict(model_marker, dataset, config_marker, extract_fts=False)\n",
    "    \n",
    "    print('- Predict 2')\n",
    "    transforms = get_transfos(size=config_marker_2.size)\n",
    "    dataset = InferenceDataset(df_val, transforms, pad=PAD, pad_advanced=PAD_ADV)\n",
    "    meter_2, _ = predict(model_marker_2, dataset, config_marker, extract_fts=False)\n",
    "        \n",
    "    print('- Update shapes')\n",
    "    dataset = InferenceDataset(df_val, None, pad=PAD, pad_advanced=PAD_ADV)\n",
    "    for i in range(len(dataset)):\n",
    "        shape = dataset[i][2]\n",
    "        meter.preds[i].update_shape(shape)\n",
    "        meter_2.preds[i].update_shape(shape)\n",
    "\n",
    "    f1s = {c: [] for c in config_marker.labels}\n",
    "    recalls = {c: [] for c in config_marker.labels}\n",
    "    \n",
    "    dataset = InferenceDataset(df_val, None, pad=False)\n",
    "    \n",
    "    print('- Evaluate')\n",
    "    for idx in range(len(dataset)):\n",
    "        img, gt, shape = dataset[idx] \n",
    "\n",
    "        gt = Boxes(gt, (shape[0], shape[1]), bbox_format=\"yolo\")['pascal_voc']\n",
    "        gt = [gt[dataset.classes[idx] == i] for i in range(len(config_marker.labels))]\n",
    "        \n",
    "        assert len(gt[-1])\n",
    "        \n",
    "        preds = [meter.preds[idx]['pascal_voc'][meter.labels[idx] == i] for i in range(len(config_marker.labels))]\n",
    "        preds_2 = [meter_2.preds[idx]['pascal_voc'][meter_2.labels[idx] == i] for i in range(len(config_marker.labels))]\n",
    "\n",
    "        scores = [meter.confidences[idx][meter.labels[idx] == i] for i in range(len(config_marker.labels))]\n",
    "        scores_2 = [meter_2.confidences[idx][meter_2.labels[idx] == i] for i in range(len(config_marker.labels))]\n",
    "        \n",
    "        boxes = np.concatenate([preds[0], preds_2[0]], 0)\n",
    "        confs = np.concatenate([scores[0], scores_2[0]], 0)\n",
    "\n",
    "        merged_boxes, merged_confs = nms(boxes, confs, threshold=0.4)\n",
    "        merged_boxes = [merged_boxes]\n",
    "        merged_boxes_list.append(merged_boxes)\n",
    "        confs_list.append(merged_confs)\n",
    "\n",
    "        for i, (t, p1, p2, pm) in enumerate(zip(gt, preds, preds_2, merged_boxes)):\n",
    "#             metrics = compute_metrics(p1, t)\n",
    "#             print(f\"Preds 1 : {metrics['f1_score'] :.3f}\")\n",
    "#             metrics = compute_metrics(p2, t)\n",
    "#             print(f\"Preds 2 : {metrics['f1_score'] :.3f}\")\n",
    "            metrics = compute_metrics(pm, t)\n",
    "#             print(f\"Preds nms : {metrics['f1_score'] :.3f}\")\n",
    "            \n",
    "            f1s[config_marker.labels[i]].append(metrics['f1_score'])\n",
    "            recalls[config_marker.labels[i]].append(metrics['recall'])\n",
    "\n",
    "    for k, v in f1s.items():\n",
    "        print(f'{k} \\t Avg F1: {np.mean(v):.3f}  \\t Avg F1==1: {np.mean(np.array(v) == 1):.3f}', end=\"\\t\")\n",
    "        print(f'Avg Recall==1: {np.mean(np.array(recalls[k]) == 1):.3f}')\n",
    "#         break\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ens best fixed :\n",
    "- point \t Avg F1: 0.933  \t Avg F1==1: 0.612\tAvg Recall==1: 0.745"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_val = df[df['split'] == \"val\"].reset_index(drop=True)\n",
    "# # df_val['path'] = f'../input/{config_marker.version}/images/valid/' + df_val['id'] + '.jpg'\n",
    "# df_val['path'] = f'../input/{config_marker.version}/val2017/' + df_val['id'] + '.jpg'\n",
    "# df_val['gt_path'] = f'../input/{config_marker.version}/labels/valid/' + df_val['id'] + '.txt'\n",
    "\n",
    "# TYPES = [\"scatter\"]\n",
    "# df_val = df_val[df_val['chart-type'].isin(TYPES)].reset_index(drop=True)\n",
    "\n",
    "# transforms = get_transfos(size=config_marker.size)\n",
    "# dataset = InferenceDataset(df_val, transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# meter_marker, _ = predict(model_marker, dataset, config_marker)\n",
    "\n",
    "# for i, p in enumerate(meter_marker.preds):\n",
    "#     p.update_shape((df_val['img_h'][i], df_val['img_w'][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chart model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdet.apis import init_detector, inference_detector  # depend heavily on mmcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHED_CLASSES = [\n",
    "    'x_title', 'y_title', 'plot_area', 'other', 'xlabel', 'ylabel',\n",
    "    'chart_title', 'x_tick', 'y_tick', 'legend_patch', 'legend_label',\n",
    "    'legend_title', 'legend_area', 'mark_label', 'value_label',\n",
    "    'y_axis_area', 'x_axis_area', 'tick_grouping'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdir = '../input/cached/work_dirs'\n",
    "config_file = wdir + '/custom.py'\n",
    "checkpoint_file = wdir + '/cascade_rcnn_swin-t_fpn_LGF_VCE_PCE_coco_focalsmoothloss/checkpoint.pth'\n",
    "\n",
    "cached_model = init_detector(config_file, checkpoint_file, device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "from transformers import TrOCRProcessor\n",
    "from transformers import VisionEncoderDecoderModel\n",
    "\n",
    "from util.ocr import ocr, post_process_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = \"microsoft/trocr-base-stage1\"\n",
    "# processor = TrOCRProcessor.from_pretrained(name)\n",
    "# ocr_model = VisionEncoderDecoderModel.from_pretrained(name).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoConfig\n",
    "# config = AutoConfig.from_pretrained(name)\n",
    "# torch.save(config, \"../output/weights/ocr/config.pth\")\n",
    "\n",
    "# processor.save_pretrained(\"../output/weights/ocr/\")\n",
    "# ocr_model.save_pretrained(\"../output/weights/ocr/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = TrOCRProcessor.from_pretrained(\"../output/weights/ocr/\")\n",
    "\n",
    "config = torch.load(\"../output/weights/ocr/config.pth\")\n",
    "\n",
    "ocr_model = VisionEncoderDecoderModel(config)\n",
    "ocr_model.load_state_dict(torch.load('../output/weights/ocr/pytorch_model.bin'))\n",
    "\n",
    "ocr_model = ocr_model.cuda().eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main\n",
    "- Enforce sim between dets\n",
    "- conv sim not robust to col  (#26)\n",
    "- Make sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT = False\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = InferenceDataset(df_val, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TO_REMOVE = [\"513147edc8a1\", \"a7a81c55df4c\", \"039d3e82ebaf\", \"82c3706f2698\", \"6d4d21bdc9a8\", \"ca30ad3528c4\", \"1ab7f626447d\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIXES = {\n",
    "    \"17000b60f53e\": [-2.36, -1.6, -1.3, -0.8, -0.5, 0.009, 0.416, 0.768, 1.296, 1.539, 2.027],\n",
    "    \"6d4d21bdc9a8\": [7400.0, 8100.0, 9300.0, 6300.0, 10000.0, 11800.0, 9800.0, 11700.0, 16128.0, 18823.0, 21519.0],\n",
    "    \"e93bed1228d6\": [ 5., 10., 15., 20., 22., 25., 26., 30., 32.],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.abs(gt.y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(0)\n",
    "\n",
    "scores = []\n",
    "for idx in range(len(dataset)):\n",
    "#     idx = 2  # 30\n",
    "#     DEBUG = True\n",
    "    \n",
    "    img, gt, _ = dataset[idx]\n",
    "    id_ = df_val.id[idx]\n",
    "    \n",
    "#     if id_ not in [\n",
    "# #         \"3f65b43556ac\",\n",
    "# #         \"464599bcd4f1\",\n",
    "# #         \"a311643a2dae\",\n",
    "# #         \"1ab7f626447d\",\n",
    "# #         \"22decba7318f\",\n",
    "# #         \"611e9fffebc3\",\n",
    "# #         \"9b62dbdded7b\",\n",
    "# #         \"bb1e2880dfea\",\n",
    "# #         \"2cfc2230ea25\",\n",
    "# #         \"5931d6d316b0\",\n",
    "# #         \"855b6cf38c6a\",\n",
    "# #         \"fbad72f7acbd\",\n",
    "# #         \"5c6466354e49\",\n",
    "#     ]:\n",
    "#         continue\n",
    "    \n",
    "    padding_bottom, padding_right = 0, 0\n",
    "    if img.shape[1] > img.shape[0] * 1.4:\n",
    "        padding_bottom = int(img.shape[1] * 0.9) - img.shape[0]\n",
    "    if img.shape[1] < img.shape[0] * 0.9:\n",
    "        padding_right = int(img.shape[0] * 1) - img.shape[1]\n",
    "\n",
    "#         continue\n",
    "#     meter.preds[idx].update_shape((img.shape[0] + padding, img.shape[1]))\n",
    "    \n",
    "#     if f1s['point'][idx] == 1:\n",
    "#         continue\n",
    "\n",
    "    if id_ in TO_REMOVE:\n",
    "        continue\n",
    "\n",
    "    print(idx, id_, end=\"\\t\")\n",
    "    title = f\"{id_} - {df_val.source[idx]} {df_val['chart-type'][idx]}\"\n",
    "    \n",
    "    preds = [[], [], [], []]\n",
    "#     preds_marker = [\n",
    "#         meter_marker.preds[idx]['pascal_voc'][meter_marker.labels[idx] == i]\n",
    "#         for i in range(len(config_marker.labels))\n",
    "#     ]\n",
    "    \n",
    "    preds_marker = [p.copy() for p in merged_boxes_list[idx]]\n",
    "    \n",
    "#     if padding_bottom and PAD:\n",
    "#         pass\n",
    "# #         preds_marker[-1][:, 1] = (\n",
    "# #             preds_marker[-1][:, 1].astype(float) * (padding_bottom + img.shape[0]) / img.shape[0]\n",
    "# #         ).astype(int)\n",
    "# #         preds_marker[-1][:, 3] = (\n",
    "# #             preds_marker[-1][:, 3].astype(float) * (padding_bottom + img.shape[0]) / img.shape[0]\n",
    "# #         ).astype(int)\n",
    "\n",
    "#     if padding_right and PAD_ADV and PAD:\n",
    "#         pass\n",
    "# #         preds_marker[-1][:, 0] = (\n",
    "# #             preds_marker[-1][:, 0].astype(float) * (padding_right + img.shape[1]) / img.shape[1]\n",
    "# #         ).astype(int)\n",
    "# #         preds_marker[-1][:, 2] = (\n",
    "# #             preds_marker[-1][:, 2].astype(float) * (padding_right + img.shape[1]) / img.shape[1]\n",
    "# #         ).astype(int)\n",
    "#     else:\n",
    "#         continue\n",
    "\n",
    "    # Cached\n",
    "    cached_result = inference_detector(cached_model, dataset.paths[idx])  # list[array]\n",
    "\n",
    "    if DEBUG:\n",
    "        for i, (r, c) in enumerate(zip(cached_result, CACHED_CLASSES)):\n",
    "            if c == \"plot_area\":\n",
    "                cached_result[i] = r[:1]\n",
    "            elif c not in ['plot_area', \"xlabel\", \"ylabel\", \"x_tick\", \"y_tick\", \"legend_area\"]:\n",
    "                cached_result[i] = np.empty((0, 5))\n",
    "\n",
    "        cached_model.show_result(\n",
    "            dataset.paths[idx],\n",
    "            cached_result,\n",
    "            out_file='../output/sample_result.jpg',\n",
    "            score_thr=0.1,\n",
    "            thickness=1,\n",
    "            font_size=5,\n",
    "        )\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        plt.imshow(cv2.imread('../output/sample_result.jpg'))\n",
    "        plt.axis(False)\n",
    "        plt.show()\n",
    "            \n",
    "    # Override with cached\n",
    "    score_th = min(0.1, cached_result[4][2, 4])\n",
    "    x_labels = cached_result[4][cached_result[4][:, -1] > score_th][:, :4].astype(int)\n",
    "\n",
    "    score_th = min(0.1, cached_result[5][2, 4])\n",
    "    y_labels = cached_result[5][cached_result[5][:, -1] > score_th][:, :4].astype(int)\n",
    "\n",
    "    score_th = min(0.1, cached_result[7][2, 4])\n",
    "    x_ticks = cached_result[7][cached_result[7][:, -1] > score_th][:, :4].astype(int)\n",
    "\n",
    "    score_th = min(0.1, cached_result[8][2, 4])\n",
    "    y_ticks = cached_result[8][cached_result[8][:, -1] > score_th][:, :4].astype(int)\n",
    "\n",
    "    preds[0] = cached_result[2][:1, :4].astype(int)\n",
    "    preds[1] = np.concatenate([x_labels, y_labels])\n",
    "    preds[2] = np.concatenate([x_ticks, y_ticks])\n",
    "    preds[3] = preds_marker[-1]\n",
    "\n",
    "    if DEBUG:\n",
    "        plot_results(img, preds, figsize=(12, 7), title=title)\n",
    "        \n",
    "    preds = post_process_point_as_tick(\n",
    "        preds, marker_conf=confs_list[idx], th=0.5, max_dist=4, max_dist_o=10, verbose=DEBUG\n",
    "    )\n",
    "    preds = post_process_preds(preds)\n",
    "    preds = post_process_arrow(preds, verbose=1)\n",
    "    \n",
    "    if DEBUG:\n",
    "        plot_results(img, preds, figsize=(12, 7), title=title)\n",
    "\n",
    "    margin = (img.shape[0] + img.shape[1]) / (2 * 20)\n",
    "    preds = restrict_on_line(preds, margin=margin)\n",
    "\n",
    "    # Visual similarity\n",
    "    img_r = img.copy()\n",
    "    try:\n",
    "        legend_area = cached_result[12][0]\n",
    "        if legend_area[-1] > 0.5:\n",
    "            legend_area = legend_area.astype(int)\n",
    "            if DEBUG:\n",
    "                print('Clear legend :', legend_area)\n",
    "            img_r[legend_area[1]:legend_area[3], legend_area[0]: legend_area[2]] = 255\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    retrieved_boxes = retrieve_missing_boxes(preds, img_r, verbose=DEBUG, min_sim=0.75, seed=0)\n",
    "    if len(retrieved_boxes):\n",
    "#         print('RETRIEVED', len(retrieved_boxes), end=\"\\t\")\n",
    "        preds[-1] = np.concatenate([preds[-1], retrieved_boxes])\n",
    "        \n",
    "    if PLOT or DEBUG:\n",
    "        plot_results(img, preds, figsize=(12, 7), title=title)\n",
    "\n",
    "    # OCR\n",
    "    x_texts = ocr(ocr_model, processor, img, preds[1], margin=1, plot=DEBUG)\n",
    "    x_values, x_errors = post_process_texts(x_texts)\n",
    "\n",
    "    if DEBUG:\n",
    "        print(\"x labels :\", x_values, \" - errors:\", x_errors)\n",
    "#     print(x_values)\n",
    "#     print(preds[3])\n",
    "    \n",
    "    if len(preds[-1]):\n",
    "        reg_x = linear_regression(preds[3], x_values, x_errors, preds[-1], mode=\"x\", verbose=DEBUG)\n",
    "\n",
    "        y_texts = ocr(ocr_model, processor, img, preds[2], margin=3, plot=DEBUG)\n",
    "        y_values, y_errors = post_process_texts(y_texts)\n",
    "\n",
    "        if DEBUG:\n",
    "             print(\"y labels :\", y_values, \" - errors:\", y_errors)\n",
    "\n",
    "        reg_y = linear_regression(preds[4], y_values, y_errors, preds[-1], mode=\"y\", verbose=DEBUG)\n",
    "\n",
    "        gt = df_target[df_target['id'] == id_].reset_index(drop=True)\n",
    "        gt[[\"x\", \"y\"]] = gt[[\"x\", \"y\"]].astype(float)\n",
    "        gt = gt.sort_values(['x', 'y'], ignore_index=True)\n",
    "        \n",
    "        if id_ in FIXES:\n",
    "            gt[\"y\"] = FIXES[id_]\n",
    "\n",
    "        reg_x = np.round(reg_x, rounding(np.max(reg_x)))\n",
    "        pred = pd.DataFrame({\"x\": reg_x, \"y\": reg_y})\n",
    "        pred = pred.sort_values(['x', 'y'], ignore_index=True)\n",
    "\n",
    "        score_x = score_series(gt['x'].values, pred['x'].values)\n",
    "        score_y = score_series(gt['y'].values, pred['y'].values)\n",
    "    else:\n",
    "        score_x, score_y = 0, 0\n",
    "\n",
    "    if len(retrieved_boxes) and DEBUG:\n",
    "        print(len(pred), \"preds,\", len(gt), \"gts\")\n",
    "\n",
    "    print(f\"Scores  -  x: {score_x:.3f}  - y: {score_y:.3f}\")\n",
    "    \n",
    "    scores += [score_x, score_y]\n",
    "    \n",
    "#     if score_x == 0 and score_y == 0:\n",
    "#         plot_results(img, preds, figsize=(12, 7), title=title)\n",
    "\n",
    "    if DEBUG:\n",
    "        print(f'GT : {len(gt)}')\n",
    "#         display(gt)\n",
    "        print(f'PRED : {len(pred)}')\n",
    "#         display(pred)\n",
    "\n",
    "    if DEBUG:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Scatter CV : {np.mean(scores) :.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done ! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "f7241b2af102f7e024509099765066b36197b195077f7bfac6e5bc041ba17c8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
